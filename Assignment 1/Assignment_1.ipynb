{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfFPpQfEPbaS",
        "outputId": "3e8567d5-b9c4-46cb-e87c-9eabbd29ee2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Qwn-T3kvvnsQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Er7LBvrvq9r",
        "outputId": "c3759776-51d7-489a-9011-9b2dff781c40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello! I am learning NLP using NLTK. I love machine learning, AI, and data science. :) #NLPRocks\"\n",
        "\n",
        "# Whitespace\n",
        "ws = WhitespaceTokenizer()\n",
        "print(\"Whitespace:\", ws.tokenize(text))\n",
        "\n",
        "# Punctuation\n",
        "print(\"WordPunct:\", wordpunct_tokenize(text))\n",
        "\n",
        "# Treebank\n",
        "tree = TreebankWordTokenizer()\n",
        "print(\"Treebank:\", tree.tokenize(text))\n",
        "\n",
        "# Tweet\n",
        "tweet = TweetTokenizer()\n",
        "print(\"Tweet:\", tweet.tokenize(text))\n",
        "\n",
        "# MWE\n",
        "mwe = MWETokenizer([('machine','learning'), ('data','science')])\n",
        "print(\"MWE:\", mwe.tokenize(text.split()))\n",
        "\n",
        "# ---------------- STEMMING ----------------\n",
        "\n",
        "words = [\"running\", \"runs\", \"studies\", \"studying\"]\n",
        "\n",
        "porter = PorterStemmer()\n",
        "print(\"Porter:\", [porter.stem(w) for w in words])\n",
        "\n",
        "snow = SnowballStemmer(\"english\")\n",
        "print(\"Snowball:\", [snow.stem(w) for w in words])\n",
        "\n",
        "# ---------------- LEMMATIZATION ----------------\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words2 = [\"running\", \"better\", \"studies\", \"cars\"]\n",
        "\n",
        "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in words2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465gSwb3vxi7",
        "outputId": "1646b9f2-1af5-4609-a75f-b73ea43f40d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace: ['Hello!', 'I', 'am', 'learning', 'NLP', 'using', 'NLTK.', 'I', 'love', 'machine', 'learning,', 'AI,', 'and', 'data', 'science.', ':)', '#NLPRocks']\n",
            "WordPunct: ['Hello', '!', 'I', 'am', 'learning', 'NLP', 'using', 'NLTK', '.', 'I', 'love', 'machine', 'learning', ',', 'AI', ',', 'and', 'data', 'science', '.', ':)', '#', 'NLPRocks']\n",
            "Treebank: ['Hello', '!', 'I', 'am', 'learning', 'NLP', 'using', 'NLTK.', 'I', 'love', 'machine', 'learning', ',', 'AI', ',', 'and', 'data', 'science.', ':', ')', '#', 'NLPRocks']\n",
            "Tweet: ['Hello', '!', 'I', 'am', 'learning', 'NLP', 'using', 'NLTK', '.', 'I', 'love', 'machine', 'learning', ',', 'AI', ',', 'and', 'data', 'science', '.', ':)', '#NLPRocks']\n",
            "MWE: ['Hello!', 'I', 'am', 'learning', 'NLP', 'using', 'NLTK.', 'I', 'love', 'machine', 'learning,', 'AI,', 'and', 'data', 'science.', ':)', '#NLPRocks']\n",
            "Porter: ['run', 'run', 'studi', 'studi']\n",
            "Snowball: ['run', 'run', 'studi', 'studi']\n",
            "Lemmatization: ['running', 'better', 'study', 'car']\n"
          ]
        }
      ]
    }
  ]
}